{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "import torch  \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from curious.utils.utils import move_paddings_to_right\n",
    "from curious.config import TrainingConfig, RLConfig, WandbConfig, BaseConfig, SamplingConfig, RewardConfig, SFLConfig \n",
    "from curious.train.training_setup import set_up_training\n",
    "from curious.train.trainer import PolicyGradientTrainer\n",
    "from curious.sampling.sampling import sequences_log_probs \n",
    "from curious.policy_gradient.loss import ActorLoss, masked_mean \n",
    "from curious.replay.experience import Experience, ReplayBuffer, join_experience_batch\n",
    "\n",
    "PAD_TOKEN_ID = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils \n",
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  1,   2,   3, 890, 891, 128, 128],\n",
       "         [  1,   2,   3,   4, 899, 900, 911]]),\n",
       " tensor([[False, False,  True,  True, False, False],\n",
       "         [False, False, False,  True,  True,  True]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 1, 1, 1],\n",
    "        [0, 1, 1, 1, 1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_ids = torch.tensor(\n",
    "    [\n",
    "        [PAD_TOKEN_ID, PAD_TOKEN_ID, 1, 2, 3,],\n",
    "        [PAD_TOKEN_ID, 1,            2, 3, 4,],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "sequence_ids = torch.tensor(\n",
    "    [\n",
    "        [PAD_TOKEN_ID, PAD_TOKEN_ID, 1, 2, 3, 890, 891, PAD_TOKEN_ID, PAD_TOKEN_ID, PAD_TOKEN_ID],\n",
    "        [PAD_TOKEN_ID, 1,            2, 3, 4, 899, 900, 911,          PAD_TOKEN_ID, PAD_TOKEN_ID],\n",
    "    ]\n",
    ")\n",
    "\n",
    "move_paddings_to_right(input_ids, attention_mask, sequence_ids, PAD_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading \n",
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Loading target policy Qwen/Qwen2-0.5B-Instruct ####\n",
      "Applied Liger kernels to Qwen2\n",
      "#### Loading dataset openai/gsm8k ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 250804.44 examples/s]\n",
      "Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 21209.50 examples/s]\n",
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 28762.80 examples/s]\n",
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 28585.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected train_max_length: 212\n",
      "Setting train_max_length to 212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 7906.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected test_max_length: 188\n",
      "Setting test_max_length to 188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 8570.12 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Loading rollout data loader ####\n",
      "#### Defining actor loss ####\n",
      "#### Defining reward model ####\n",
      "#### Defining generation config ####\n",
      "#### Defining evaluation config ####\n",
      "#### Defining optimizer ####\n",
      "#### Defining lr scheduler ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_config = TrainingConfig(\n",
    "    rl_config=RLConfig(\n",
    "        group_size=3, \n",
    "        mini_batch_size=6,\n",
    "        kl_weight=0,\n",
    "        \n",
    "    ),\n",
    "    wandb_config=WandbConfig(),\n",
    "    base_config=BaseConfig(\n",
    "        train_batch_size=2,\n",
    "        \n",
    "    ),\n",
    "    sampling_config=SamplingConfig(\n",
    "        max_new_tokens=256,\n",
    "    ),\n",
    "    reward_config=RewardConfig(),\n",
    "    sfl_config=SFLConfig(\n",
    "    ),\n",
    ")\n",
    "#train_config.rl_config\n",
    "training_setup, init_train_state = set_up_training(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['run_name', 'device', 'seed', 'model', 'optimizer', 'lr_scheduler', 'reference_model', 'kl_controller'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_train_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PolicyGradientTrainer(training_setup) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = next(iter(trainer.rollout_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] or:\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] to include these operations in the captured graph.\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] \n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] Graph break: from user code at:\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     return func(*args, **kwargs)\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/liger_kernel/transformers/model/qwen2.py\", line 188, in lce_forward\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     outputs = self.model(\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/transformers/utils/generic.py\", line 965, in wrapper\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     output = func(self, *args, **kwargs)\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 519, in forward\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     causal_mask = self._update_causal_mask(\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 589, in _update_causal_mask\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] \n",
      "W0511 17:05:28.711000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] \n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0511 17:06:06.578000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:906] [9/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0511 17:06:06.578000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:906] [9/8]    function: 'forward' (/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:152)\n",
      "W0511 17:06:06.578000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:906] [9/8]    last reason: 9/0: not L['past_key_value'].key_cache                           \n",
      "W0511 17:06:06.578000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:906] [9/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0511 17:06:06.578000 4901 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:906] [9/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin gc.collect. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
      "  torch._dynamo.utils.warn_once(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Batch indx <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Batch indx \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.cuda.memory_allocated: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>951519GB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "torch.cuda.memory_allocated: \u001b[1;36m0.\u001b[0m951519GB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.cuda.memory_reserved: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>658203GB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "torch.cuda.memory_reserved: \u001b[1;36m1.\u001b[0m658203GB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.cuda.max_memory_reserved: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>945312GB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "torch.cuda.max_memory_reserved: \u001b[1;36m1.\u001b[0m945312GB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_batch_returns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.58203125</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/max_batch_returns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/min_batch_returns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_batch_solved_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1669921875</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_batch_format_returns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/max_batch_format_returns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/min_batch_format_returns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_batch_outcome_returns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5833333333333334</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/max_batch_outcome_returns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/min_batch_outcome_returns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_batch_length_penalty'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/max_batch_length_penalty'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/min_batch_length_penalty'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_num_words_in_completions'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.84375</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/max_num_words_in_completions'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/min_num_words_in_completions'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-06</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_action_entropy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.318359375</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_batches_visited'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'train/mean_batch_returns'\u001b[0m: \u001b[1;36m-0.58203125\u001b[0m,\n",
       "    \u001b[32m'train/max_batch_returns'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "    \u001b[32m'train/min_batch_returns'\u001b[0m: \u001b[1;36m-1.0\u001b[0m,\n",
       "    \u001b[32m'train/mean_batch_solved_rate'\u001b[0m: \u001b[1;36m0.1669921875\u001b[0m,\n",
       "    \u001b[32m'train/mean_batch_format_returns'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[32m'train/max_batch_format_returns'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[32m'train/min_batch_format_returns'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[32m'train/mean_batch_outcome_returns'\u001b[0m: \u001b[1;36m-0.5833333333333334\u001b[0m,\n",
       "    \u001b[32m'train/max_batch_outcome_returns'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "    \u001b[32m'train/min_batch_outcome_returns'\u001b[0m: \u001b[1;36m-1.0\u001b[0m,\n",
       "    \u001b[32m'train/mean_batch_length_penalty'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[32m'train/max_batch_length_penalty'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[32m'train/min_batch_length_penalty'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[32m'train/mean_num_words_in_completions'\u001b[0m: \u001b[1;36m7.84375\u001b[0m,\n",
       "    \u001b[32m'train/max_num_words_in_completions'\u001b[0m: \u001b[1;36m30.0\u001b[0m,\n",
       "    \u001b[32m'train/min_num_words_in_completions'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "    \u001b[32m'train/lr'\u001b[0m: \u001b[1;36m1e-06\u001b[0m,\n",
       "    \u001b[32m'train/mean_action_entropy'\u001b[0m: \u001b[1;36m0.318359375\u001b[0m,\n",
       "    \u001b[32m'num_batches_visited'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "replay_buffer = trainer.collect_trajectories(\n",
    "    init_train_state, \n",
    "    batch_inputs, \n",
    "    0 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = replay_buffer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = trainer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences -> torch.Size([278])\n",
      "action_log_probs -> torch.Size([277])\n",
      "returns -> torch.Size([])\n",
      "solved_mask -> torch.Size([])\n",
      "advantages -> torch.Size([])\n",
      "attention_mask -> torch.Size([278])\n",
      "action_mask -> torch.Size([277])\n"
     ]
    }
   ],
   "source": [
    "for key in exp.keys:\n",
    "    val = getattr(exp, key)\n",
    "    if val is not None:\n",
    "        print(key, '->', val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Please reason step by step, and put your final answer within $\\boxed{}$.<|im_end|>\n",
      "<|im_start|>user\n",
      "Nala found 5 seashells at the beach. The next day, she found another 7, and the following day, she found twice the number she got from the first two days. How many seashells does Nala have?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Nala found 5 seashells on the first day and 7 seashells on the second day. So, the total number of seashells she has found so far is $5 + 7 = 12$.\n",
      "\n",
      "On the following day, she found twice the number she got from the first two days. So, the number of seashells she found on the second day is $2 \\times 12 = 24$.\n",
      "\n",
      "To find the total number of seashells Nala has, we add the number she found on the first two days and the number she found on the following day. That is $12 + 24 = 36$.\n",
      "The answer is: $\\boxed{36}$<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(exp.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Please reason step by step, and put your final answer within $\\boxed{}$.<|im_end|>\n",
      "<|im_start|>user\n",
      "Nala found 5 seashells at the beach. The next day, she found another 7, and the following day, she found twice the number she got from the first two days. How many seashells does Nala have?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Nala found 5 seashells on the first day and 7 seashells on the second day. So, the total number of seashells she has found so far is $5 + 7 = 12$.\n",
      "\n",
      "On the following day, she found twice the number she got from the first two days. So, the number of seashells she found on the second day is $2 \\times 12 = 24$.\n",
      "\n",
      "To find the total number of seashells Nala has, we add the number she found on the first two days and the number she found on the following day. That is $12 + 24 = 36$.\n",
      "The answer is: $\\boxed{36}$<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.decode(\n",
    "        exp.sequences[exp.attention_mask]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(exp.returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(exp.solved_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1172, dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(exp.advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.bfloat16)\n",
      "tensor(-0.5000, dtype=torch.bfloat16)\n",
      "tensor(-0.3203, dtype=torch.bfloat16)\n",
      "<|im_start|>system\n",
      "Please reason step by step, and put your final answer within $\\boxed{}$.<|im_end|>\n",
      "<|im_start|>user\n",
      "Nala found 5 seashells at the beach. The next day, she found another 7, and the following day, she found twice the number she got from the first two days. How many seashells does Nala have?<|im_end|>\n",
      "<|im_start|>assistant\n",
      " Nala found 5 seashells on the first day.\n",
      "She found 7 seashells on the second day.\n",
      "Let's call the number of seashells she found on the third day $x$.\n",
      "So, the total number of seashells she has after three days is $5 + 7 + x$.\n",
      "We know that the total number of seashells she found is 5 + 7 + $x$.\n",
      "So, we have the equation $5 + 7 + x = 5 + 7 + x + x$.\n",
      "Simplifying the equation, we get $12 + x = 12 + 2x$.\n",
      "Subtracting $12$ from both sides gives $x = 2x$.\n",
      "So, the number of seashells she found each day is $x = 2$.\n",
      "Therefore, Nala has 2 seashells.\n",
      "The answer is: $\\boxed{2}$<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "exp1 = replay_buffer[1]\n",
    "print(exp1.solved_mask)\n",
    "print(exp1.returns)\n",
    "print(exp1.advantages)\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        exp1.sequences[exp1.attention_mask]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.bfloat16)\n",
      "tensor(-1., dtype=torch.bfloat16)\n",
      "tensor(-0.8008, dtype=torch.bfloat16)\n",
      "<|im_start|>system\n",
      "Please reason step by step, and put your final answer within $\\boxed{}$.<|im_end|>\n",
      "<|im_start|>user\n",
      "Nala found 5 seashells at the beach. The next day, she found another 7, and the following day, she found twice the number she got from the first two days. How many seashells does Nala have?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find out how many seashells Nala has in total, we first need to calculate the number of seashells she found each day:\n",
      "\n",
      "1. On the first day, she found 5 seashells.\n",
      "2. On the second day, she found 7 more seashells than the first day.\n",
      "3. On the third day, she found twice the number she got from the first two days, which means \\(2 \\times 5 + 7 = 10 + 7 = 17\\) seashells.\n",
      "\n",
      "Adding these up, we get the total number of seashells Nala has:\n",
      "\n",
      "\\(5 + 7 + 17 = 39\\) seashells.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "exp2 = replay_buffer[2]\n",
    "print(exp2.solved_mask)\n",
    "print(exp2.returns)\n",
    "print(exp2.advantages)\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        exp2.sequences[exp2.attention_mask]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.bfloat16)\n",
      "tensor(-1., dtype=torch.bfloat16)\n",
      "tensor(0., dtype=torch.bfloat16)\n",
      "<|im_start|>system\n",
      "Please reason step by step, and put your final answer within $\\boxed{}$.<|im_end|>\n",
      "<|im_start|>user\n",
      "Zig wrote four times as many books as Flo. If Zig wrote 60 books, how many books did they write altogether?<|im_end|>\n",
      "<|im_start|>assistant\n",
      " If Zig wrote 60 books, and Zig wrote four times as many books as Flo, then Flo wrote 60/4 = 15 books.\n",
      "To find out how many books they wrote altogether, we add the number of books Zig wrote and the number of books Flo wrote. So, 60 + 15 = 75 books.\n",
      "So, they wrote 75 books altogether.\n",
      "The answer is $\\boxed{75}$.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "exp3 = replay_buffer[3]\n",
    "print(exp3.solved_mask)\n",
    "print(exp3.returns)\n",
    "print(exp3.advantages)\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        exp3.sequences[exp3.attention_mask]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " If Zig wrote 60 books, and Zig wrote four times as many books as Flo, then Flo wrote 60/4 = 15 books.\n",
      "To find out how many books they wrote altogether, we add the number of books Zig wrote and the number of books Flo wrote. So, 60 + 15 = 75 books.\n",
      "So, they wrote 75 books altogether.\n",
      "The answer is $\\boxed{75}$.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.decode(\n",
    "        exp3.sequences[1:][exp3.action_mask]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_sampler = DataLoader(\n",
    "    replay_buffer,\n",
    "    batch_size=trainer.rl_config.mini_batch_size,\n",
    "    shuffle= False,\n",
    "    drop_last=False,\n",
    "    collate_fn=join_experience_batch,\n",
    "    num_workers=trainer.base_config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_exp = next(iter(experience_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6]), torch.Size([6]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_exp.returns.shape, next_exp.advantages.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences -> torch.Size([6, 278])\n",
      "action_log_probs -> torch.Size([6, 277])\n",
      "returns -> torch.Size([6])\n",
      "solved_mask -> torch.Size([6])\n",
      "advantages -> torch.Size([6])\n",
      "attention_mask -> torch.Size([6, 278])\n",
      "action_mask -> torch.Size([6, 277])\n"
     ]
    }
   ],
   "source": [
    "for key in next_exp.keys:\n",
    "    val = getattr(next_exp, key)\n",
    "    if val is not None:\n",
    "        print(key, '->', val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_exp = next_exp.to(init_train_state[\"model\"].device) \n",
    "log_probs, _ = sequences_log_probs(\n",
    "    init_train_state[\"model\"], \n",
    "    sequence_ids= next_exp.sequences, \n",
    "    attention_mask= next_exp.attention_mask,\n",
    "    return_entropy=False,\n",
    "    logits_minibatch_size=trainer.rl_config.logits_minibatch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 277])\n"
     ]
    }
   ],
   "source": [
    "print(log_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "loss, mean_kl, mean_actor_loss = trainer.actor_loss(\n",
    "    log_probs=log_probs, \n",
    "    experience= next_exp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0010, device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<CompiledFunctionBackward>)\n",
      "0\n",
      "tensor([[-0.4805, -1.3438, -1.2266,  ..., -1.1172, -1.1172, -1.1172],\n",
      "        [ 0.2559,  0.4121,  0.3516,  ...,  0.3203,  0.3203,  0.3203],\n",
      "        [ 0.6406,  1.0312,  0.8789,  ...,  0.8008,  0.8008,  0.8008],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(mean_kl)\n",
    "print(mean_actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 277])\n"
     ]
    }
   ],
   "source": [
    "print(mean_actor_loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0010, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_mean(\n",
    "    mean_actor_loss,\n",
    "    mask = next_exp.action_mask,\n",
    "    dim = -1,\n",
    "    use_fixed_response_length = False \n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
