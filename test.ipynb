{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "import torch  \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from curious.utils.utils import move_paddings_to_right\n",
    "from curious.config import TrainingConfig, RLConfig, WandbConfig, BaseConfig, SamplingConfig, RewardConfig, SFLConfig \n",
    "from curious.train.training_setup import set_up_training\n",
    "from curious.train.trainer import PolicyGradientTrainer\n",
    "from curious.sampling.sampling import sequences_log_probs \n",
    "from curious.policy_gradient.loss import ActorLoss\n",
    "from curious.replay.experience import Experience, ReplayBuffer, join_experience_batch\n",
    "\n",
    "PAD_TOKEN_ID = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils \n",
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  1,   2,   3, 890, 891, 128, 128],\n",
       "         [  1,   2,   3,   4, 899, 900, 911]]),\n",
       " tensor([[False, False,  True,  True, False, False],\n",
       "         [False, False, False,  True,  True,  True]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 1, 1, 1],\n",
    "        [0, 1, 1, 1, 1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_ids = torch.tensor(\n",
    "    [\n",
    "        [PAD_TOKEN_ID, PAD_TOKEN_ID, 1, 2, 3,],\n",
    "        [PAD_TOKEN_ID, 1,            2, 3, 4,],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "sequence_ids = torch.tensor(\n",
    "    [\n",
    "        [PAD_TOKEN_ID, PAD_TOKEN_ID, 1, 2, 3, 890, 891, PAD_TOKEN_ID, PAD_TOKEN_ID, PAD_TOKEN_ID],\n",
    "        [PAD_TOKEN_ID, 1,            2, 3, 4, 899, 900, 911,          PAD_TOKEN_ID, PAD_TOKEN_ID],\n",
    "    ]\n",
    ")\n",
    "\n",
    "move_paddings_to_right(input_ids, attention_mask, sequence_ids, PAD_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading \n",
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Loading target policy Qwen/Qwen2-0.5B-Instruct ####\n",
      "Applied Liger kernels to Qwen2\n",
      "#### Loading dataset openai/gsm8k ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 157982.45 examples/s]\n",
      "Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 271363.47 examples/s]\n",
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 16842.80 examples/s]\n",
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 16191.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected train_max_length: 212\n",
      "Setting train_max_length to 212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7473/7473 [00:01<00:00, 4720.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected test_max_length: 188\n",
      "Setting test_max_length to 188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 5350.43 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Loading rollout data loader ####\n",
      "#### Loading KL controller ####\n",
      "#### Loading reference model ####\n",
      "Applied Liger kernels to Qwen2\n",
      "#### Defining actor loss ####\n",
      "#### Defining reward model ####\n",
      "#### Defining generation config ####\n",
      "#### Defining evaluation config ####\n",
      "#### Defining optimizer ####\n",
      "#### Defining lr scheduler ####\n"
     ]
    }
   ],
   "source": [
    "train_config = TrainingConfig(\n",
    "    rl_config=RLConfig(\n",
    "        group_size=3, \n",
    "        mini_batch_size=6,\n",
    "        \n",
    "    ),\n",
    "    wandb_config=WandbConfig(),\n",
    "    base_config=BaseConfig(\n",
    "        train_batch_size=2,\n",
    "        \n",
    "    ),\n",
    "    sampling_config=SamplingConfig(\n",
    "        max_new_tokens=256,\n",
    "    ),\n",
    "    reward_config=RewardConfig(),\n",
    "    sfl_config=SFLConfig(\n",
    "    ),\n",
    ")\n",
    "#train_config.rl_config\n",
    "training_setup, init_train_state = set_up_training(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['run_name', 'device', 'seed', 'model', 'optimizer', 'lr_scheduler', 'reference_model', 'kl_controller'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_train_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PolicyGradientTrainer(training_setup) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = next(iter(trainer.rollout_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] or:\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] to include these operations in the captured graph.\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] \n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] Graph break: from user code at:\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     return func(*args, **kwargs)\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/liger_kernel/transformers/model/qwen2.py\", line 188, in lce_forward\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     outputs = self.model(\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/transformers/utils/generic.py\", line 965, in wrapper\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     output = func(self, *args, **kwargs)\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 519, in forward\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     causal_mask = self._update_causal_mask(\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 589, in _update_causal_mask\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] \n",
      "W0511 09:25:09.122000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/tensor.py:869] [0/0] \n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0511 09:26:13.221000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:906] [9/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0511 09:26:13.221000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:906] [9/8]    function: 'forward' (/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:152)\n",
      "W0511 09:26:13.221000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:906] [9/8]    last reason: 9/0: not L['past_key_value'].key_cache                           \n",
      "W0511 09:26:13.221000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:906] [9/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0511 09:26:13.221000 5754 /system/conda/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:906] [9/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin gc.collect. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
      "  torch._dynamo.utils.warn_once(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Batch indx <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Batch indx \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.cuda.memory_allocated: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>849268GB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "torch.cuda.memory_allocated: \u001b[1;36m1.\u001b[0m849268GB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.cuda.memory_reserved: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.</span>371094GB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "torch.cuda.memory_reserved: \u001b[1;36m2.\u001b[0m371094GB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.cuda.max_memory_reserved: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.</span>406250GB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "torch.cuda.max_memory_reserved: \u001b[1;36m2.\u001b[0m406250GB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_batch_returns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.66796875</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_batch_solved_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1669921875</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_num_words_in_completions'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1640625</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/max_num_words_in_completions'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/min_num_words_in_completions'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_batch_format_returns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_batch_outcome_returns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6666666666666666</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_batch_length_penalty'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-06</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'train/mean_action_entropy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.169921875</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'num_batches_visited'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'train/mean_batch_returns'\u001b[0m: \u001b[1;36m-0.66796875\u001b[0m,\n",
       "    \u001b[32m'train/mean_batch_solved_rate'\u001b[0m: \u001b[1;36m0.1669921875\u001b[0m,\n",
       "    \u001b[32m'train/mean_num_words_in_completions'\u001b[0m: \u001b[1;36m1.1640625\u001b[0m,\n",
       "    \u001b[32m'train/max_num_words_in_completions'\u001b[0m: \u001b[1;36m2.0\u001b[0m,\n",
       "    \u001b[32m'train/min_num_words_in_completions'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "    \u001b[32m'train/mean_batch_format_returns'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[32m'train/mean_batch_outcome_returns'\u001b[0m: \u001b[1;36m-0.6666666666666666\u001b[0m,\n",
       "    \u001b[32m'train/mean_batch_length_penalty'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "    \u001b[32m'train/lr'\u001b[0m: \u001b[1;36m1e-06\u001b[0m,\n",
       "    \u001b[32m'train/mean_action_entropy'\u001b[0m: \u001b[1;36m0.169921875\u001b[0m,\n",
       "    \u001b[32m'num_batches_visited'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "replay_buffer = trainer.collect_trajectories(\n",
    "    init_train_state, \n",
    "    batch_inputs, \n",
    "    0 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = replay_buffer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = trainer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences -> torch.Size([220])\n",
      "action_log_probs -> torch.Size([219])\n",
      "log_probs_ref -> torch.Size([219])\n",
      "returns -> torch.Size([])\n",
      "solved_mask -> torch.Size([])\n",
      "advantages -> torch.Size([])\n",
      "attention_mask -> torch.Size([220])\n",
      "action_mask -> torch.Size([219])\n"
     ]
    }
   ],
   "source": [
    "for key in exp.keys:\n",
    "    val = getattr(exp, key)\n",
    "    if val is not None:\n",
    "        print(key, '->', val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Please reason step by step, and put your final answer within $\\boxed{}$.<|im_end|>\n",
      "<|im_start|>user\n",
      "A pencil costs $0.5 each and a folder costs $0.9 each. An office needs two dozen pencils and 20 pieces of folders. How much does it cost to buy the office supplies?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Two dozen pencils is equal to 2 x 12 = 24 pencils.\n",
      "The cost of the pencils is $0.5 x 24 = $12.\n",
      "The cost of the folders is $0.9 x 20 = $18.\n",
      "Therefore, the total cost to buy the office supplies is $12 + $18 = $30.\n",
      "The answer is: $\\boxed{30}$<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(exp.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Please reason step by step, and put your final answer within $\\boxed{}$.<|im_end|>\n",
      "<|im_start|>user\n",
      "A pencil costs $0.5 each and a folder costs $0.9 each. An office needs two dozen pencils and 20 pieces of folders. How much does it cost to buy the office supplies?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Two dozen pencils is equal to 2 x 12 = 24 pencils.\n",
      "The cost of the pencils is $0.5 x 24 = $12.\n",
      "The cost of the folders is $0.9 x 20 = $18.\n",
      "Therefore, the total cost to buy the office supplies is $12 + $18 = $30.\n",
      "The answer is: $\\boxed{30}$<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.decode(\n",
    "        exp.sequences[exp.attention_mask]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1., dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(exp.returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(exp.solved_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(exp.advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.bfloat16)\n",
      "tensor(-1., dtype=torch.bfloat16)\n",
      "tensor(0., dtype=torch.bfloat16)\n",
      "<|im_start|>system\n",
      "Please reason step by step, and put your final answer within $\\boxed{}$.<|im_end|>\n",
      "<|im_start|>user\n",
      "A pencil costs $0.5 each and a folder costs $0.9 each. An office needs two dozen pencils and 20 pieces of folders. How much does it cost to buy the office supplies?<|im_end|>\n",
      "<|im_start|>assistant\n",
      " Two dozen pencils is equal to 2 x 12 = 24 pencils.\n",
      "If each pencil costs $0.5, then the total cost for the pencils is 24 x $0.5 = $12.\n",
      "The office needs 20 folders, and each folder costs $0.9, so the total cost for the folders is 20 x $0.9 = $18.\n",
      "To find the total cost for the office supplies, we add the cost of the pencils and the cost of the folders, so $12 + $18 = $30.\n",
      "The answer is $\\boxed{30}$.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "exp1 = replay_buffer[1]\n",
    "print(exp1.solved_mask)\n",
    "print(exp1.returns)\n",
    "print(exp1.advantages)\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        exp1.sequences[exp1.attention_mask]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.bfloat16)\n",
      "tensor(-1., dtype=torch.bfloat16)\n",
      "tensor(0., dtype=torch.bfloat16)\n",
      "<|im_start|>system\n",
      "Please reason step by step, and put your final answer within $\\boxed{}$.<|im_end|>\n",
      "<|im_start|>user\n",
      "A pencil costs $0.5 each and a folder costs $0.9 each. An office needs two dozen pencils and 20 pieces of folders. How much does it cost to buy the office supplies?<|im_end|>\n",
      "<|im_start|>assistant\n",
      " Two dozen pencils is equal to 2 x 12 = 24 pencils.\n",
      "The cost of one pencil is $0.5, so the cost of 24 pencils is 24 x $0.5 = $12.\n",
      "The cost of one folder is $0.9, so the cost of 20 folders is 20 x $0.9 = $18.\n",
      "To find the total cost, we add the cost of the pencils and the cost of the folders, so $12 + $18 = $30.\n",
      "The answer is $\\boxed{30}$.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "exp2 = replay_buffer[2]\n",
    "print(exp2.solved_mask)\n",
    "print(exp2.returns)\n",
    "print(exp2.advantages)\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        exp2.sequences[exp2.attention_mask]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., dtype=torch.bfloat16)\n",
      "tensor(-1., dtype=torch.bfloat16)\n",
      "tensor(-0.5781, dtype=torch.bfloat16)\n",
      "<|im_start|>system\n",
      "Please reason step by step, and put your final answer within $\\boxed{}$.<|im_end|>\n",
      "<|im_start|>user\n",
      "Jackson had 20 kilograms of meat. He used 1/4 of the meat to make meatballs and used 3 kilograms of meat to make spring rolls. How many kilograms of meat are left?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Jackson used 20*1/4 = 5 kilograms of meat for meatballs.\n",
      "Jackson used 3 kilograms of meat for spring rolls.\n",
      "So, Jackson used 5+3 = 8 kilograms of meat in total.\n",
      "Jackson has 20-8 = 12 kilograms of meat left.\n",
      "The answer is: $\\boxed{12}$<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "exp3 = replay_buffer[3]\n",
    "print(exp3.solved_mask)\n",
    "print(exp3.returns)\n",
    "print(exp3.advantages)\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        exp3.sequences[exp3.attention_mask]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jackson used 20*1/4 = 5 kilograms of meat for meatballs.\n",
      "Jackson used 3 kilograms of meat for spring rolls.\n",
      "So, Jackson used 5+3 = 8 kilograms of meat in total.\n",
      "Jackson has 20-8 = 12 kilograms of meat left.\n",
      "The answer is: $\\boxed{12}$<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.decode(\n",
    "        exp3.sequences[1:][exp3.action_mask]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_sampler = DataLoader(\n",
    "    replay_buffer,\n",
    "    batch_size=trainer.rl_config.mini_batch_size,\n",
    "    shuffle= False,\n",
    "    drop_last=False,\n",
    "    collate_fn=join_experience_batch,\n",
    "    num_workers=trainer.base_config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_exp = next(iter(experience_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6]), torch.Size([6]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_exp.returns.shape, next_exp.advantages.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences -> torch.Size([6, 220])\n",
      "action_log_probs -> torch.Size([6, 219])\n",
      "log_probs_ref -> torch.Size([6, 219])\n",
      "returns -> torch.Size([6])\n",
      "solved_mask -> torch.Size([6])\n",
      "advantages -> torch.Size([6])\n",
      "attention_mask -> torch.Size([6, 220])\n",
      "action_mask -> torch.Size([6, 219])\n"
     ]
    }
   ],
   "source": [
    "for key in next_exp.keys:\n",
    "    val = getattr(next_exp, key)\n",
    "    if val is not None:\n",
    "        print(key, '->', val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_exp = next_exp.to(init_train_state[\"model\"].device) \n",
    "log_probs, _ = sequences_log_probs(\n",
    "    init_train_state[\"model\"], \n",
    "    sequence_ids= next_exp.sequences, \n",
    "    attention_mask= next_exp.attention_mask,\n",
    "    return_entropy=False,\n",
    "    logits_minibatch_size=trainer.rl_config.logits_minibatch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 219])\n"
     ]
    }
   ],
   "source": [
    "print(log_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "loss, mean_kl, mean_actor_loss = trainer.actor_loss(\n",
    "    log_probs=log_probs, \n",
    "    experience= next_exp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0003, device='cuda:0', grad_fn=<CompiledFunctionBackward>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<CompiledFunctionBackward>)\n",
      "tensor(-0.0864, device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(mean_kl)\n",
    "print(mean_actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
